diff --git a/app-conf/elephant.conf b/app-conf/elephant.conf
index 67c9bfc..c29bb8a 100644
--- a/app-conf/elephant.conf
+++ b/app-conf/elephant.conf
@@ -17,4 +17,4 @@ enable_analytics=false
 
 metrics=false
 
-metrics_agent_jar="-javaagent:/export/apps/elephant/dr-agent/dr-agent.jar=app-name=dr-elephant-test,app-fabric=prod-ltx1,app-host=ltx1-hcl0582.grid.linkedin.com,app-port=8080,amf-host=ltx1-amf.prod.linkedin.com"
\ No newline at end of file
+//metrics_agent_jar="-javaagent:/export/apps/elephant/dr-agent/dr-agent.jar=app-name=dr-elephant-test,app-fabric=prod-ltx1,app-host=ltx1-hcl0582.grid.linkedin.com,app-port=8080,amf-host=ltx1-amf.prod.linkedin.com"
\ No newline at end of file
diff --git a/app/com/linkedin/drelephant/ElephantRunner.java b/app/com/linkedin/drelephant/ElephantRunner.java
index f6726a2..f9ac8ed 100644
--- a/app/com/linkedin/drelephant/ElephantRunner.java
+++ b/app/com/linkedin/drelephant/ElephantRunner.java
@@ -26,6 +26,7 @@ import com.linkedin.drelephant.analysis.AnalyticJobGeneratorHadoop2;
 
 import com.linkedin.drelephant.security.HadoopSecurity;
 
+import controllers.Application;
 import controllers.MetricsController;
 import java.io.IOException;
 import java.security.PrivilegedAction;
@@ -55,6 +56,8 @@ public class ElephantRunner implements Runnable {
   private static final int EXECUTOR_NUM = 5;                // The number of executor threads to analyse the jobs
   //TODO : REMOVE AFTER TESTING!
   private int deleteCount = 0;
+  private int otherwiseSkipped = 0;
+  private int analysedJobs = 0;
 
   private static final String FETCH_INTERVAL_KEY = "drelephant.analysis.fetch.interval";
   private static final String RETRY_INTERVAL_KEY = "drelephant.analysis.retry.interval";
@@ -182,6 +185,7 @@ public class ElephantRunner implements Runnable {
         logger.info(String.format("Analysis of %s took %sms", analysisName, processingTime));
         MetricsController.setJobProcessingTime(processingTime);
         MetricsController.markProcessedJobs();
+        analysedJobs++;
 
       } catch (InterruptedException e) {
         logger.info("Thread interrupted");
@@ -190,6 +194,7 @@ public class ElephantRunner implements Runnable {
 
         Thread.currentThread().interrupt();
       } catch (Exception e) {
+        //logger.info("*****************THIS IS THE EXCEPTION AFTER GETTING EXCEPTION");
         logger.error(e.getMessage());
         logger.error(ExceptionUtils.getStackTrace(e));
 
@@ -197,13 +202,16 @@ public class ElephantRunner implements Runnable {
           logger.error("Add analytic job id [" + _analyticJob.getAppId() + "] into the retry list.");
           _analyticJobGenerator.addIntoRetries(_analyticJob);
         }
-        if (_analyticJob != null && _analyticJob.secondRetry()) {
+        else if (_analyticJob != null && _analyticJob.secondRetry()) {
+          if(_analyticJob.get_secondRetries() ==1) {
+            otherwiseSkipped++;
+          }
           logger.error("Add analytic job id [" + _analyticJob.getAppId() + "] into the second retry list.");
           _analyticJobGenerator.addIntoSecondRetryQueue(_analyticJob);
         } else {
           if (_analyticJob != null) {
             MetricsController.markSkippedJob();
-            logger.info("********************* SKIPPED JOB!!!! \n\n *****************JOBS SKIPPED : "+ (++deleteCount));
+            logger.info("********************* SKIPPED JOB!!!! \n\n *****************JOBS SKIPPED : "+ (++deleteCount)+"  OTHERWISE SKIPPED IS : "+otherwiseSkipped+"\nJOB SKIPPED IS :"+_analyticJob.getAppId());
             logger.error("Drop the analytic job. Reason: reached the max retries for application id = ["
                     + _analyticJob.getAppId() + "].");
           }
diff --git a/app/com/linkedin/drelephant/analysis/AnalyticJob.java b/app/com/linkedin/drelephant/analysis/AnalyticJob.java
index 1c0b357..d333595 100644
--- a/app/com/linkedin/drelephant/analysis/AnalyticJob.java
+++ b/app/com/linkedin/drelephant/analysis/AnalyticJob.java
@@ -37,16 +37,16 @@ public class AnalyticJob {
 
   private static final String UNKNOWN_JOB_TYPE = "Unknown";   // The default job type when the data matches nothing.
   private static final int _RETRY_LIMIT = 3;                  // Number of times a job needs to be tried before dropping
-  private static final int _SECOND_RETRY_LIMIT = 3;
+  private static final int _SECOND_RETRY_LIMIT = 5;
   private static final String EXCLUDE_JOBTYPE = "exclude_jobtypes_filter"; // excluded Job Types for heuristic
 
 
-  public boolean readyForRetry() {
+  public boolean readyForSecondRetry() {
     this._timeLeftToRetry = this._timeLeftToRetry - 1;
     return (this._timeLeftToRetry <= 0);
   }
 
-  public AnalyticJob setTimeToRetry() {
+  public AnalyticJob setTimeToSecondRetry() {
     this._timeLeftToRetry = (this._secondRetries)*5;
     return this;
   }
@@ -54,6 +54,10 @@ public class AnalyticJob {
   private int _timeLeftToRetry;
   private int _retries = 0;
 
+  public int get_secondRetries() {
+    return _secondRetries;
+  }
+
   private int _secondRetries = 0;
   private ApplicationType _type;
   private String _appId;
diff --git a/app/com/linkedin/drelephant/analysis/AnalyticJobGeneratorHadoop2.java b/app/com/linkedin/drelephant/analysis/AnalyticJobGeneratorHadoop2.java
index cfc2149..fabc2ef 100644
--- a/app/com/linkedin/drelephant/analysis/AnalyticJobGeneratorHadoop2.java
+++ b/app/com/linkedin/drelephant/analysis/AnalyticJobGeneratorHadoop2.java
@@ -22,11 +22,7 @@ import controllers.MetricsController;
 import java.io.IOException;
 import java.net.HttpURLConnection;
 import java.net.URL;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.List;
-import java.util.Queue;
-import java.util.Random;
+import java.util.*;
 import java.util.concurrent.ConcurrentLinkedQueue;
 import models.AppResult;
 import org.apache.hadoop.conf.Configuration;
@@ -166,14 +162,24 @@ public class AnalyticJobGeneratorHadoop2 implements AnalyticJobGenerator {
       appList.add(_retryQueue.poll());
     }
 
-    if(!_secondRetryQueue.isEmpty()) {
-      _secondRetryQueue.removeIf((job) -> {
-        boolean remove = job.readyForRetry();
-        if(remove) {
-          appList.add(job);
-        }
-        return remove;
-      });
+    Iterator iteratorSecondRetry = _secondRetryQueue.iterator();
+    //while
+
+    while (iteratorSecondRetry.hasNext()) {
+      AnalyticJob job = (AnalyticJob) iteratorSecondRetry.next();
+      if(job.readyForSecondRetry()) {
+        appList.add(job);
+        iteratorSecondRetry.remove();
+      }
+    }
+//    if(!_secondRetryQueue.isEmpty()) {
+//      _secondRetryQueue.removeIf((job) -> {
+//        boolean remove = job.readyForSecondRetry();
+//        if(remove) {
+//          appList.add(job);
+//        }
+//        return remove;
+//      });}
 
 //      _secondRetryQueue.forEach((job) -> {
 //
@@ -189,9 +195,10 @@ public class AnalyticJobGeneratorHadoop2 implements AnalyticJobGenerator {
 //
 //        }
 //      });
-    }
+
 
     _lastTime = _currentTime;
+    logger.info("\n\n****************SIZE OF ANALYTIC QUEUE IS : "+ appList.size());
     return appList;
   }
 
@@ -205,7 +212,7 @@ public class AnalyticJobGeneratorHadoop2 implements AnalyticJobGenerator {
 
   @Override
   public void addIntoSecondRetryQueue(AnalyticJob promise) {
-    _secondRetryQueue.add(promise.setTimeToRetry());
+    _secondRetryQueue.add(promise.setTimeToSecondRetry());
     int secondRetryQueueSize = _secondRetryQueue.size();
     MetricsController.setSecondRetryQueueSize(secondRetryQueueSize);
     //MetricsController.setRetryQueueSize(secondRetryQueueSize);
diff --git a/app/com/linkedin/drelephant/spark/heuristics/JvmUsedMemoryHeuristic.scala b/app/com/linkedin/drelephant/spark/heuristics/JvmUsedMemoryHeuristic.scala
index 94b83dd..e495ae5 100644
--- a/app/com/linkedin/drelephant/spark/heuristics/JvmUsedMemoryHeuristic.scala
+++ b/app/com/linkedin/drelephant/spark/heuristics/JvmUsedMemoryHeuristic.scala
@@ -88,8 +88,8 @@ object JvmUsedMemoryHeuristic {
       _.peakJvmUsedMemory.getOrElse(JVM_USED_MEMORY, 0L).asInstanceOf[Number].longValue
     }.sortWith(_< _).drop(executorList.size/2).head
     lazy val maxExecutorPeakJvmUsedMemory: Long = (executorList.map {
-      _.peakJvmUsedMemory.get(JVM_USED_MEMORY)
-    }.max).getOrElse(0L)
+      _.peakJvmUsedMemory.getOrElse(JVM_USED_MEMORY, 0).asInstanceOf[Number].longValue
+    }.max)
 
     val DEFAULT_MAX_EXECUTOR_PEAK_JVM_USED_MEMORY_THRESHOLDS =
       SeverityThresholds(low = 1.5 * (maxExecutorPeakJvmUsedMemory + reservedMemory), moderate = 2 * (maxExecutorPeakJvmUsedMemory + reservedMemory), severe = 4 * (maxExecutorPeakJvmUsedMemory + reservedMemory), critical = 8 * (maxExecutorPeakJvmUsedMemory + reservedMemory), ascending = true)
diff --git a/app/com/linkedin/drelephant/spark/heuristics/UnifiedMemoryHeuristic.scala b/app/com/linkedin/drelephant/spark/heuristics/UnifiedMemoryHeuristic.scala
index df8d98c..5a40c98 100644
--- a/app/com/linkedin/drelephant/spark/heuristics/UnifiedMemoryHeuristic.scala
+++ b/app/com/linkedin/drelephant/spark/heuristics/UnifiedMemoryHeuristic.scala
@@ -41,16 +41,13 @@ class UnifiedMemoryHeuristic(private val heuristicConfigurationData: HeuristicCo
     val evaluator = new Evaluator(this, data)
 
     var resultDetails = Seq(
-      new HeuristicResultDetails("Max peak unified memory", evaluator.maxUnifiedMemory.toString),
+      new HeuristicResultDetails("Allocated memory for the unified region", evaluator.maxMemory.toString),
       new HeuristicResultDetails("Mean peak unified memory", evaluator.meanUnifiedMemory.toString)
     )
 
-    if (evaluator.severityPeak.getValue > Severity.LOW.getValue) {
+    if (evaluator.severity.getValue > Severity.LOW.getValue) {
       resultDetails = resultDetails :+ new HeuristicResultDetails("Note", "The value of peak unified memory is very low, we recommend to decrease spark.memory.fraction, or total executor memory")
     }
-    if (evaluator.severitySkew.getValue > Severity.LOW.getValue) {
-      resultDetails = resultDetails :+ new HeuristicResultDetails("Note", "There is an imbalance in the amount of memory used by executors, please look into this to see if it can be distributed more evenly")
-    }
     val result = new HeuristicResult(
       heuristicConfigurationData.getClassName,
       heuristicConfigurationData.getHeuristicName,
@@ -72,7 +69,7 @@ object UnifiedMemoryHeuristic {
       data.appConfigurationProperties
 
     lazy val executorSummaries: Seq[ExecutorSummary] = data.executorSummaries
-    val executorList : Seq[ExecutorSummary] = executorSummaries.filterNot(_.id.equals("driver"))
+    val executorList: Seq[ExecutorSummary] = executorSummaries.filterNot(_.id.equals("driver"))
 
     //allocated memory for the unified region
     val maxMemory: Long = executorList.head.maxMemory
@@ -80,25 +77,19 @@ object UnifiedMemoryHeuristic {
     val DEFAULT_PEAK_UNIFIED_MEMORY_THRESHOLDS =
       SeverityThresholds(low = 0.7 * maxMemory, moderate = 0.6 * maxMemory, severe = 0.4 * maxMemory, critical = 0.2 * maxMemory, ascending = false)
 
-    val DEFAULT_UNIFIED_MEMORY_SKEW_THRESHOLDS =
-      SeverityThresholds(low = 1.5 * meanUnifiedMemory, moderate = 2 * meanUnifiedMemory, severe = 4 * meanUnifiedMemory, critical = 8 * meanUnifiedMemory, ascending = true)
-
     def getPeakUnifiedMemoryExecutorSeverity(executorSummary: ExecutorSummary): Severity = {
       return DEFAULT_PEAK_UNIFIED_MEMORY_THRESHOLDS.severityOf(executorSummary.peakUnifiedMemory.getOrElse(EXECUTION_MEMORY, 0).asInstanceOf[Number].longValue
         + executorSummary.peakUnifiedMemory.getOrElse(STORAGE_MEMORY, 0).asInstanceOf[Number].longValue)
     }
 
     lazy val meanUnifiedMemory: Long = (executorList.map {
-      executorSummary=> {executorSummary.peakUnifiedMemory.getOrElse(EXECUTION_MEMORY, 0).asInstanceOf[Number].longValue
-      + executorSummary.peakUnifiedMemory.getOrElse(STORAGE_MEMORY, 0).asInstanceOf[Number].longValue}
+      executorSummary => {
+        executorSummary.peakUnifiedMemory.getOrElse(EXECUTION_MEMORY, 0).asInstanceOf[Number].longValue
+        + executorSummary.peakUnifiedMemory.getOrElse(STORAGE_MEMORY, 0).asInstanceOf[Number].longValue
+      }
     }.sum) / executorList.size
-    lazy val maxUnifiedMemory: Long = executorList.map {
-      executorSummary => {executorSummary.peakUnifiedMemory.getOrElse(EXECUTION_MEMORY, 0).asInstanceOf[Number].longValue
-      + executorSummary.peakUnifiedMemory.getOrElse(STORAGE_MEMORY, 0).asInstanceOf[Number].longValue}
-    }.max
-    val severitySkew = DEFAULT_UNIFIED_MEMORY_SKEW_THRESHOLDS.severityOf(maxUnifiedMemory)
 
-    lazy val severityPeak: Severity = {
+    lazy val severity: Severity = {
       var severityPeakUnifiedMemoryVariable: Severity = Severity.NONE
       for (executorSummary <- executorList) {
         var peakUnifiedMemoryExecutorSeverity: Severity = getPeakUnifiedMemoryExecutorSeverity(executorSummary)
@@ -108,6 +99,6 @@ object UnifiedMemoryHeuristic {
       }
       severityPeakUnifiedMemoryVariable
     }
-    lazy val severity: Severity = Severity.max(severityPeak, severitySkew)
   }
+
 }
diff --git a/test/com/linkedin/drelephant/spark/heuristics/JvmUsedMemoryHeuristicTest.scala b/test/com/linkedin/drelephant/spark/heuristics/JvmUsedMemoryHeuristicTest.scala
index e228df6..b783171 100644
--- a/test/com/linkedin/drelephant/spark/heuristics/JvmUsedMemoryHeuristicTest.scala
+++ b/test/com/linkedin/drelephant/spark/heuristics/JvmUsedMemoryHeuristicTest.scala
@@ -20,11 +20,11 @@
 //  val appConfigurationProperties = Map("spark.driver.memory"->"40000000000", "spark.executor.memory"->"500000000")
 //
 //  val executorData = Seq(
-//    newDummyExecutorData("1", Map("jvmUsedMemory" -> 394567123)),
+//    newDummyExecutorData("1", Map("" -> 394567123)),
 //    newDummyExecutorData("2", Map("jvmUsedMemory" -> 23456834)),
 //    newDummyExecutorData("3", Map("jvmUsedMemory" -> 334569)),
 //    newDummyExecutorData("4", Map("jvmUsedMemory" -> 134563)),
-//    newDummyExecutorData("5", Map("jvmUsedMemory" -> 234564)),
+//    newDummyExecutorData("5", Map("" -> 234564)),
 //    newDummyExecutorData("driver", Map("jvmUsedMemory" -> 394561))
 //  )
 //  describe(".apply") {
@@ -90,7 +90,8 @@
 //    totalShuffleWrite = 0,
 //    maxMemory = 0,
 //    executorLogs = Map.empty,
-//    peakJvmUsedMemory
+//    peakJvmUsedMemory,
+//    peakUnifiedMemory = Map.empty
 //  )
 //
 //  def newFakeSparkApplicationData(
@@ -107,7 +108,8 @@
 //      new ApplicationInfoImpl(appId, name = "app", Seq.empty),
 //      jobDatas = Seq.empty,
 //      stageDatas = Seq.empty,
-//      executorSummaries = executorSummaries
+//      executorSummaries = executorSummaries,
+//      stagesWithFailedTasks = Seq.empty
 //    )
 //
 //    SparkApplicationData(appId, restDerivedData, Some(logDerivedData))
